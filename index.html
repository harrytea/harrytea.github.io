<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yonghui Wang's Homepage - ‰∏≠ÂõΩÁßëÂ≠¶ÊäÄÊúØÂ§ßÂ≠¶</title>
  <meta name="author" content="Yonghui Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Yonghui Wang (ÁéãÂãáÊÉ†)</name>
                  </p>
                  <p>I am currently a 3rd-year Ph.D. student at <a href="https://en.ustc.edu.cn/">University of Science
                      and Technology of China (USTC)</a>, supervised by
                    <a href="http://staff.ustc.edu.cn/~lihq/en/"> Prof. Houqiang Li </a> and <a
                      href="http://staff.ustc.edu.cn/~zhwg/"> Prof. Wengang Zhou </a>.
                    Before that, I received my Bachelor degree from <a href="https://www.ouc.edu.cn/">Ocean University
                      of China (OUC)</a> in 2021.
                  </p>
                  <p>
                    My research interests are about computer vision and deep learning, especially on multimodal large
                    language models.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:wangyh1206@gmail.com">Email</a> &nbsp/&nbsp
                    <a target="_blank" href="https://scholar.google.com.hk/citations?user=GGMWna4AAAAJ&hl=zh-CN">Google
                      Scholar</a> &nbsp/&nbsp
                    <a target="_blank" href="https://github.com/harrytea">Github</a> &nbsp/&nbsp
                    <a target="_blank" href="./images/CV_YonghuiWang.pdf ">CV</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:100%;max-width:100%">
                  <a target="_blank" href="images/YonghuiWang.jpg"><img style="width:60%;max-width:80%"
                      alt="profile photo" src="images/YonghuiWang.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>




          <!--Publications-->
          <h2>Publications (* indicates equal contribution)</h2>
          <hr>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <!--ROOT-->
              <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/root.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                  <p>
                    <a target="_blank" href="https://arxiv.org/abs/2411.15714">
                      <papertitle>
                        ROOT: VLM based System for Indoor Scene Understanding and Beyond
                      </papertitle>
                    </a>
                    <br>
                    <b>Yonghui Wang</b>, Shi-Yong Chen, Zhenxing Zhou, Siyi Li, Haoran Li, Wengang Zhou, Houqiang Li
                    <br>
                    <em>arXiv</em>, 2024
                    <br>
                    [<a target="_blank" href="https://arxiv.org/abs/2411.15714">paper</a>]
                    [<a target="_blank" href="https://github.com/harrytea/ROOT">code</a>]
                  </p>
                </td>
              </tr>
              <!--AdaptVision-->
              <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/adaptvision.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                  <p>
                    <a target="_blank" href="https://arxiv.org/abs/2408.16986">
                      <papertitle>
                        AdaptVision: Dynamic Input Scaling in MLLMs for Versatile Scene Understanding
                      </papertitle>
                    </a>
                    <br>
                    <b>Yonghui Wang</b>, Wengang Zhou, Hao Feng, Houqiang Li
                    <br>
                    <em>arXiv</em>, 2024
                    <br>
                    [<a target="_blank" href="https://arxiv.org/abs/2408.16986">paper</a>]
                    [<a target="_blank" href="https://github.com/harrytea/AdaptVision">code</a>]
                  </p>
                </td>
              </tr>
              <!--TGDoc-->
              <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/tgdoc.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                  <p>
                    <a target="_blank" href="https://arxiv.org/abs/2311.13194">
                      <papertitle>
                        Towards improving document understanding: An exploration on text-grounding via mllms
                      </papertitle>
                    </a>
                    <br>
                    <b>Yonghui Wang</b>, Wengang Zhou, Hao Feng, Keyi Zhou, Houqiang Li
                    <br>
                    <em>arXiv</em>, 2023
                    <br>
                    [<a target="_blank" href="https://arxiv.org/abs/2311.13194">paper</a>]
                    [<a target="_blank" href="https://github.com/harrytea/TGDoc">code</a>]
                  </p>
                </td>
              </tr>
              <!--SwinShadow-->
              <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/swinshadow.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                  <p>
                    <a target="_blank" href="https://arxiv.org/abs/2408.03521">
                      <papertitle>
                        SwinShadow: Shifted Window for Ambiguous Adjacent Shadow Detection
                      </papertitle>
                    </a>
                    <br>
                    <b>Yonghui Wang</b>, Shaokai Liu, Li Li, Wengang Zhou, Houqiang Li
                    <br>
                    <em>ACM Transactions on Multimedia Computing, Communications, and Applications (ACM TOMM)</em>, 2024
                    <br>
                    [<a target="_blank" href="https://arxiv.org/abs/2408.03521">paper</a>]
                    [<a target="_blank" href="https://github.com/harrytea/SwinShadow">code</a>]
                  </p>
                </td>
              </tr>

              <!--Detect Any Shadow-->
              <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/shadowsam.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                  <p>
                    <a target="_blank" href="https://arxiv.org/abs/2305.16698">
                      <papertitle>
                        Detect Any Shadow: Segment Anything for Video Shadow Detection
                      </papertitle>
                    </a>
                    <br>
                    <b>Yonghui Wang</b>, Wengang Zhou, Yunyao Mao, Houqiang Li
                    <br>
                    <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em>, 2023
                    <br>
                    [<a target="_blank" href="https://arxiv.org/abs/2305.16698">paper</a>]
                    [<a target="_blank" href="https://github.com/harrytea/Detect-AnyShadow">code</a>]
                    <br>
                  </p>

                </td>
              </tr>
              <!--PRNet-->
              <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/prnet.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                  <p>
                    <a target="_blank" href="https://arxiv.org/abs/2311.00455">
                      <papertitle>
                        Progressive Recurrent Network for shadow removal
                      </papertitle>
                    </a>
                    <br>
                    <b>Yonghui Wang</b>, Wengang Zhou, Hao Feng, Li Li, Houqiang Li
                    <br>
                    <em>Computer Vision and Image Understanding (CVIU)</em>, 2023
                    <br>
                    [<a target="_blank" href="https://arxiv.org/abs/2311.00455">paper</a>]
                    [<a target="_blank" href="https://github.com/harrytea/PRNet">code</a>]
                  </p>
                </td>
              </tr>

              <!--UDoc-GAN-->
              <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/udoc_gan.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                  <p>
                    <a target="_blank" href="https://arxiv.org/abs/2210.08216">
                      <papertitle>
                        UDoc-GAN: Unpaired Document Illumination Correction with Background Light Prior
                      </papertitle>
                    </a>
                    <br>
                    <b>Yonghui Wang</b>, Wengang Zhou, Zhenbo Lu, Houqiang Li
                    <br>
                    <em>30th ACM International Conference on Multimedia (ACM MM)</em>, 2022

                    <br>
                    [<a target="_blank" href="https://arxiv.org/abs/2210.08216">paper</a>]
                    [<a target="_blank" href="https://github.com/harrytea/UDoc-GAN">code</a>]
                    <br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!--New Section-->
          <!--Papers I Co-authored-->
          <h2>Papers I Co-authored</h2>
          <hr>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr bgcolor="#ffffff">
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/textcot.png' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                  <p>
                    <a target="_blank" href="https://arxiv.org/abs/2404.09797">
                      <papertitle>
                        TextCoT: Zoom In for Enhanced Multimodal Text-Rich Image Understanding
                      </papertitle>
                    </a>
                    <br>
                    Bozhi Luan, Hao Feng, Hong Chen, <b>Yonghui Wang</b>, Wengang Zhou, Houqiang Li
                    <br>
                    <em>arXiv</em>, 2024
                    <br>
                    [<a target="_blank" href="https://arxiv.org/abs/2404.09797">paper</a>]
                    [<a target="_blank" href="https://github.com/bzluan/TextCoT">code</a>]
                    <br>
                  </p>
                </td>
              </tr>
              <tr bgcolor="#ffffff"></tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/lanetca.png' width="250"></div>
              </td>
              <td width="75%" valign="middle">
                <p>
                  <a target="_blank" href="https://arxiv.org/abs/2408.13852">
                    <papertitle>
                      LaneTCA: Enhancing Video Lane Detection with Temporal Context Aggregation
                    </papertitle>
                  </a>
                  <br>
                  Keyi Zhou, Li Li, Wengang Zhou, <b>Yonghui Wang</b>, Hao Feng, Houqiang Li
                  <br>
                  <em>arXiv</em>, 2024
                  <br>
                  [<a target="_blank" href="https://arxiv.org/abs/2408.13852">paper</a>]
                  [<a target="_blank" href="https://github.com/Alex-1337/LaneTCA">code</a>]
                  <br>
                </p>
              </td>
      </tr>
    </tbody>
  </table>

  <!--Awards and Honors-->
  <h2>Awards and Honors</h2>
  <hr>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <ul>
          <li>
            National Scholarship for Graduate Students.
            <div style="float:right; text-align:right">2023</div>
          </li>
          <li>
            Longfor Scholarship.
            <div style="float:right; text-align:right">2022</div>
          </li>
          <li>
            Outstanding graduates of OUC.
            <div style="float:right; text-align:right">2021</div>
          </li>
          <li>
            Second Prize in the National Undergraduate Mathematics Competition
            <div style="float:right; text-align:right">2018</div>
          </li>
          <li>
            First Prize in the Shandong Undergraduate Mathematics Competition
            <div style="float:right; text-align:right">2018</div>
          </li>
        </ul>
      </tr>
    </tbody>
  </table>

  <!--Research Experience-->
  <h2>Research Experience</h2>
  <hr>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <ul>
          <li><b>[2024/05 ~ now] </b> &nbsp &nbsp Research Intern, Game AI Center, Tencent IEG
            (Mentor: <a href="https://scholar.google.com/citations?user=BIFXUFYAAAAJ&hl=en">Shi-Yong Chen</a>).
          </li>
        </ul>
      </tr>
    </tbody>
  </table>

  <!--            Academic Services-->
  <h2>Academic Services</h2>
  <hr>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <ul>
          <li>Conference Reviewer: ACM MM.</li>
          <li>Journal Reviewer: TOMM, TMM, TCSVT.</li>

        </ul>
      </tr>
    </tbody>
  </table>

  <!--           CopyRight-->
  <table style="width:50%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="text-align:center">
          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=NGBmCAmPxqNktUgrsQnH18HvCje7PCX3gYUYjyhAHyg&cl=ffffff&w=a"></script>
        </td>
      </tr>
    </tbody>
  </table>
  </td>
  </tr>
  </table>
</body>

</html>
